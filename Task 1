Task 1 
Use Python  libraries like BeautifulSoup or Scrapy to extract data from websites. 
● Identify and collect relevant datasets from public web pages.
● If you don’t code, use automated tools like Octoparse or ParseHub.
● Learn to handle HTML structure and web navigation to gather accurate data.
● Create custom datasets tailored to specific analysis needs.

Solution
Using Python with BeautifulSoup
pip install requests beautifulsoup4
Extracting all headlines from a news site
import requests
from bs4 import BeautifulSoup
# Step 1: Fetch the webpage
url = "https://www.bbc.com/news"
response = requests.get(url)
# Step 2: Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')
# Step 3: Gather all the headlines
headlines = soup.find_all('h3')
for headline in headlines:
print(headline.text.strip())
Using Scrapy (Advanced)
pip install scrapy
Scrapy Project Structure
scrapy startproject news_scraper
cd news_scraper
scrapy genspider bbc bbc.com
import scrapy
class BbcSpider(scrapy.Spider):
name = "bbc"
start_urls = ['https://www.bbc.com/news']
def parse(self, response):
for headline in response.css('h3'):
yield {'headline': headline.css('::text').get()}
